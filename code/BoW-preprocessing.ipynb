{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"name":"NB-preprocessing.ipynb","provenance":[{"file_id":"1wIhjXj1d3mUBy9wPw3Hk5KkaiRhGBfh0","timestamp":1589357566092},{"file_id":"1Ju99UwHHnLNay0IVulnI-A5m-HUD0iIm","timestamp":1589357518759}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"}},"cells":[{"cell_type":"markdown","metadata":{"id":"GUpLGjrwjVd-","colab_type":"text"},"source":["# Preprocessing for BoW\n","Available preprocessing steps:\n","* Tokenizaion\n","* Stopword removal\n","* Stemming\n","* Lemmatization\n","* nGram\n","\n","\n","```.pkl``` files were used to save space\n","\n"]},{"cell_type":"code","metadata":{"id":"SlN7vufmYAe9","colab_type":"code","outputId":"556819f6-1691-4992-b081-6d24ebdcfa99","executionInfo":{"status":"ok","timestamp":1589790623508,"user_tz":-120,"elapsed":1068,"user":{"displayName":"Samuel Knaus","photoUrl":"","userId":"12623331742292020930"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pHctgBQXz-54","colab_type":"text"},"source":["## Imports and method definitions\n","This cell needs to be executed before creating preprocessed sets. It provides imports and functions for our preprocessing methods."]},{"cell_type":"code","metadata":{"id":"cuw5AC6ukE1d","colab_type":"code","outputId":"5e28f42b-29d9-43c7-f57b-12f0ff38e41f","executionInfo":{"status":"ok","timestamp":1589790629481,"user_tz":-120,"elapsed":1577,"user":{"displayName":"Samuel Knaus","photoUrl":"","userId":"12623331742292020930"}},"colab":{"base_uri":"https://localhost:8080/","height":161}},"source":["import pandas as pd\n","import numpy as np\n","import time\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.stem.porter import PorterStemmer\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","import re, string\n","nltk.download('averaged_perceptron_tagger')\n","from nltk.corpus import wordnet as wn\n","from nltk.stem.wordnet import WordNetLemmatizer\n","from nltk import pos_tag\n","from collections import defaultdict\n","from collections.abc import Iterable\n","from nltk import ngrams\n","nltk.download('wordnet')\n","import matplotlib.pyplot as plt\n","\n","\n","###\n","### Tokenization\n","###\n","def tokenize(text, sentenceSeperate=False, includePunctation=False, excludeSpecPuct=[]):\n","    data = []\n","\n","    # intern functions\n","    def withPunctation(text):\n","        temp = []\n","        # delete unwanted punctuation\n","        for delPunct in excludeSpecPuct:\n","            text = text.replace(delPunct, \" \")\n","        # help tokenization with replacing some untokenized punctations\n","        for puct in [\"-\", \"/\", \"—\"]:\n","            text = text.replace(puct, \" \" + puct + \" \")\n","        # tokenize the sentence into words\n","        for j in word_tokenize(text):\n","            temp.append(j)\n","        return temp\n","\n","    def withoutPunctation(text):\n","        token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")  # split on whitespace (and remove punctation)\n","        return token_pattern.findall(text)\n","\n","    text = text.lower()\n","\n","    if sentenceSeperate:\n","        # iterate through each sentence in the file\n","        for sentence in sent_tokenize(text):\n","            if includePunctation:\n","                data.append(withPunctation(sentence))\n","            else:\n","                data.append(withoutPunctation(sentence))\n","    else:\n","        if includePunctation:\n","            data = withPunctation(text)\n","        else:\n","            data = withoutPunctation(text)\n","    return data\n","\n","\n","###\n","### Stopword removal\n","###\n","def removeStopwords(wordArray):\n","    my_stopwords = set(stopwords.words('english'))\n","    withoutStopwords = []\n","\n","    # test if its a list of words or a list of sentences with words\n","    if len(wordArray) > 0 and isinstance(wordArray[0], Iterable) and not isinstance(wordArray[0], str):\n","        for sentence in wordArray:\n","            withoutStopwords.append(removeStopwords(sentence))\n","\n","    else:\n","        for item in wordArray:\n","            if item not in my_stopwords:\n","                withoutStopwords.append(item)\n","    return withoutStopwords\n","\n","\n","###\n","### Stemming\n","###\n","def applyStemming(wordArray):\n","    stemmer = PorterStemmer()\n","    stems = []\n","\n","    # test if its a list of words or a list of sentences with words\n","    if len(wordArray) > 0 and isinstance(wordArray[0], Iterable) and not isinstance(wordArray[0], str):\n","        for sentence in wordArray:\n","            stems.append(applyStemming(sentence))\n","    else:\n","        for item in wordArray:\n","            stems.append(stemmer.stem(item))\n","    return stems\n","\n","\n","###\n","### Lemmatizing\n","###\n","def applyLemmatizing(\n","        wordArray):  # Quelle (stark verändert): https://www.guru99.com/stemming-lemmatization-python-nltk.html\n","    tag_map = defaultdict(lambda: wn.NOUN)\n","    tag_map['J'] = wn.ADJ\n","    tag_map['V'] = wn.VERB\n","    tag_map['R'] = wn.ADV\n","\n","    # intern function\n","    def lemmazizeText(text):\n","        temp = []\n","        for token, tag in pos_tag(text):\n","            temp.append(lemma_function.lemmatize(token, tag_map[tag[0]]))\n","        return temp\n","\n","    lemma_function = WordNetLemmatizer()\n","    baseWords = []\n","    if len(wordArray) > 0 and isinstance(wordArray[0], Iterable) and not isinstance(wordArray[0], str):\n","        for sentence in wordArray:\n","            baseWords.append(lemmazizeText(sentence))\n","    else:\n","        baseWords = lemmazizeText(wordArray)\n","\n","    return baseWords\n","\n","\n","###\n","### nGram\n","###\n","def addNGram(wordArray, NGramLength=2):\n","    holetext = wordArray\n","    temp = []\n","    if len(wordArray) > 0 and isinstance(wordArray[0], Iterable) and not isinstance(wordArray[0], str):\n","        print(\"drin\")\n","        for sentence in wordArray:\n","            temp.append(' '.join(sentence))\n","        holetext = (' '.join(temp)).split()\n","    nGrams = list(ngrams(holetext, NGramLength))\n","\n","    # make nGram from two words to one\n","    nGramsFull = pd.Series(nGrams).apply(lambda row: ' '.join(row))\n","    wordArrayCopy = wordArray.copy()\n","    wordArrayCopy.extend(nGramsFull)\n","    return (wordArrayCopy)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xLQ71LGFjVeG","colab_type":"text"},"source":["## Create preprocessed set\n"]},{"cell_type":"markdown","metadata":{"id":"bWOuaxsU96Ng","colab_type":"text"},"source":["### Load unprocessed data\n"]},{"cell_type":"code","metadata":{"id":"NZkfKIux-KpT","colab_type":"code","outputId":"e22dd3c0-f128-43bd-b301-ff26f5c79370","executionInfo":{"status":"ok","timestamp":1589790638585,"user_tz":-120,"elapsed":2582,"user":{"displayName":"Samuel Knaus","photoUrl":"","userId":"12623331742292020930"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["firstTime = time.time()\n","df = pd.read_pickle(\"drive/My Drive/Feature_generated_sets/raw/Hotel_reviews_features_selected.pkl\")\n","print (\"Loaded data in: %s seconds\" % round(time.time()-firstTime,4))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Loaded data in: 1.8443 seconds\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SHd8wIc9-PHm","colab_type":"text"},"source":["### Define preprocessing steps"]},{"cell_type":"code","metadata":{"id":"3WX4nUZajVeG","colab_type":"code","colab":{}},"source":["###\n","### This dictionary was used to create two BoW sets that we ran our Algorithms\n","### on and compared the results to different feature sets.\n","###\n","preprocessing_for_all = {\n","    \"token\": True,\n","    \"token_sentenceSeperate\": False,\n","    \"token_includePunctation\": False,\n","    \"token_excludeSpecPuct\": [],\n","    \"rem_stpwrds\": True,\n","    \"stemm\": True,\n","    \"lemmatize\": False,\n","    \"nGram\": False,\n","    \"nGram_length\":2\n","}\n","\n","###\n","### Tried out feature sets with 53 and 1550 words with this preprocessing\n","###\n","no_stemming = {\n","    \"token\": True,\n","    \"token_sentenceSeperate\": False,\n","    \"token_includePunctation\": False,\n","    \"token_excludeSpecPuct\": [],\n","    \"rem_stpwrds\": True,\n","    \"stemm\": False,\n","    \"lemmatize\": False,\n","    \"nGram\": False,\n","    \"nGram_length\":2\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7pZEB8qU-cnL","colab_type":"text"},"source":["### 3. Apply preprocessing sets"]},{"cell_type":"code","metadata":{"id":"jBUILvOD-gbj","colab_type":"code","outputId":"d08ac3f9-e8cb-48d5-ca02-e0c0456ffc51","executionInfo":{"status":"ok","timestamp":1589790739660,"user_tz":-120,"elapsed":74062,"user":{"displayName":"Samuel Knaus","photoUrl":"","userId":"12623331742292020930"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["def preprocess(review, dict): # uses the dict above to apply preprocessing\n","    if dict[\"token\"]:\n","        review = tokenize(review, sentenceSeperate=dict[\"token_sentenceSeperate\"],\n","                          includePunctation=dict[\"token_includePunctation\"],\n","                          excludeSpecPuct=dict[\"token_excludeSpecPuct\"])\n","    if dict[\"rem_stpwrds\"]:\n","        review = removeStopwords(review)\n","    if dict[\"stemm\"]:\n","        review = applyStemming(review)\n","    if dict[\"lemmatize\"]:\n","        review = applyLemmatizing(review)\n","    if dict[\"nGram\"]:\n","        review = addNGram(review, NGramLength=dict[\"nGram_length\"])\n","\n","    return review\n","\n","def createPreprocessing(dict, output_path, sharedFolder = False):\n","    df[\"Review\"] = df[\"Review\"].apply(lambda review: preprocess(review, dict))\n","    df.to_pickle(output_path)\n","\n","\n","firstTime = time.time()\n","output_path = \"no_stemming.pkl\" # Decides where to save the preprocessed set to\n","createPreprocessing(no_stemming, output_path) # Executes preprocessing based on 'dict' and 'output_path' values\n","print (\"Generated preprocessed set in: %s seconds\" % round(time.time()-firstTime,4))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Generated preprocessed set in: 73.2665 seconds\n"],"name":"stdout"}]}]}