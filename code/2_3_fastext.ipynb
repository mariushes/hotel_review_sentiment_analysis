{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sister"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing\n",
    "import pandas as pd\n",
    "import sister\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing with only tokenization and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess whole dataset\n",
    "df = pd.read_csv(\"../data/Hotel_reviews_features_selected.csv\")\n",
    "\n",
    "dict = {\n",
    "    \"token\": True, #mandatory True\n",
    "    \"token_sentenceSeperate\":False,\n",
    "    \"token_includePunctation\":False,\n",
    "    \"token_excludeSpecPuct\" :[],\n",
    "    \"remStpwrds\": False,\n",
    "    \"stemm\": False,\n",
    "    \"lemmatize\": True,\n",
    "    \"nGram\": False,\n",
    "    \"nGram_length\":2\n",
    "}\n",
    "pre_processed_data_path = \"../data/preprocessed/feature_generated/fasttext/fast_text_\"+\"_\".join(str(key) + str(value) for key, value in dict.items())+\".csv\"\n",
    "df[\"Review\"] = df[\"Review\"].apply(lambda review: preprocessing.preprocess(review,dict))\n",
    "df.to_csv(pre_processed_data_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess 1000 from dataset\n",
    "df = pd.read_csv(\"../data/Hotel_reviews_features_selected.csv\")\n",
    "df = df.head(1000)\n",
    "dict = {\n",
    "    \"token\": True, #mandatory True\n",
    "    \"token_sentenceSeperate\":False,\n",
    "    \"token_includePunctation\":False,\n",
    "    \"token_excludeSpecPuct\" :[],\n",
    "    \"remStpwrds\": False,\n",
    "    \"stemm\": False,\n",
    "    \"lemmatize\": True,\n",
    "    \"nGram\": False,\n",
    "    \"nGram_length\":2\n",
    "}\n",
    "pre_processed_limit_data_path = \"../data/preprocessed/feature_generated/fasttext/fast_text_limit_\"+\"_\".join(str(key) + str(value) for key, value in dict.items())+\".csv\"\n",
    "df[\"Review\"] = df[\"Review\"].apply(lambda review: preprocessing.preprocess(review,dict))\n",
    "df.to_csv(pre_processed_limit_data_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocessing with only tokenization, stop-word removal and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess whole dataset\n",
    "df = pd.read_csv(\"../data/Hotel_reviews_features_selected.csv\")\n",
    "\n",
    "dict = {\n",
    "    \"token\": True, #mandatory True\n",
    "    \"token_sentenceSeperate\":False,\n",
    "    \"token_includePunctation\":False,\n",
    "    \"token_excludeSpecPuct\" :[],\n",
    "    \"remStpwrds\": True,\n",
    "    \"stemm\": False,\n",
    "    \"lemmatize\": True,\n",
    "    \"nGram\": False,\n",
    "    \"nGram_length\":2\n",
    "}\n",
    "pre_processed_swr_data_path = \"../data/preprocessed/feature_generated/fasttext/fast_text_\"+\"_\".join(str(key) + str(value) for key, value in dict.items())+\".csv\"\n",
    "df[\"Review\"] = df[\"Review\"].apply(lambda review: preprocessing.preprocess(review,dict))\n",
    "df.to_csv(pre_processed_swr_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess 1000 from dataset\n",
    "df = pd.read_csv(\"../data/Hotel_reviews_features_selected.csv\")\n",
    "df = df.head(1000)\n",
    "dict = {\n",
    "    \"token\": True, #mandatory True\n",
    "    \"token_sentenceSeperate\":False,\n",
    "    \"token_includePunctation\":False,\n",
    "    \"token_excludeSpecPuct\" :[],\n",
    "    \"remStpwrds\": True,\n",
    "    \"stemm\": False,\n",
    "    \"lemmatize\": True,\n",
    "    \"nGram\": False,\n",
    "    \"nGram_length\":2\n",
    "}\n",
    "pre_processed_swr_limit_data_path = \"../data/preprocessed/feature_generated/fasttext/fast_text_limit_\"+\"_\".join(str(key) + str(value) for key, value in dict.items())+\".csv\"\n",
    "df[\"Review\"] = df[\"Review\"].apply(lambda review: preprocessing.preprocess(review,dict))\n",
    "df.to_csv(pre_processed_swr_limit_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test sister library\n",
    "Sister provides an embedder that generates feature vectors for text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sister library\n",
    "embedder = sister.MeanEmbedding(lang=\"en\")\n",
    "\n",
    "test_word = [\"pizza\", \"is\", \"like\", \"my\", \"familiy\"]\n",
    "vector = embedder(\" \".join([x for x in test_word])  )\n",
    "print(vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word to vector approach\n",
    "We tested an approach which generates a vector for each token in a review.\n",
    "Due to the high storage and computing costs we decided to generate a vector for each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A general function that converts a word list to a list of embedding vectors with the parameter embedder\n",
    "def generate_vector_list(wordList, embedder):\n",
    "    result_vector_list = [] \n",
    "    for word in wordList:\n",
    "        embedding = embedder(word)\n",
    "        result_vector_list.append(embedding)\n",
    "    return result_vector_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A try with limited data of 1000 to generate the feature took 30 minutes and 1 GB storage\n",
    "embedder = sister.MeanEmbedding(lang=\"en\")\n",
    "df = pd.read_csv(pre_processed_limit_data_path)\n",
    "start = time.process_time()\n",
    "#df[\"Review\"] = df[\"Review\"].apply(lambda review: generate_vector_list(review, embedder))\n",
    "#df.to_csv(\"../data/preprocessed/feature_generated/fasttext/fast_text_limit_unigram_features.csv\")\n",
    "print(time.process_time() - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review to vector approach\n",
    "This is the actual feature generation where we generated one vector for each review.\n",
    "Resulting in circa 2 GB of data in 40 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing (tokenization, lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = sister.MeanEmbedding(lang=\"en\")\n",
    "df = pd.read_csv(pre_processed_data_path)\n",
    "start = time.process_time()\n",
    "df[\"Review\"] = df[\"Review\"].apply(lambda review: embedder(\" \".join([x for x in review])))\n",
    "df.to_csv(\"../data/preprocessed/feature_generated/fasttext/fast_text_nonswr_features.csv\")\n",
    "display(df)\n",
    "print(time.process_time() - start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocessing (tokenization, lemmatization, stop-word removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = sister.MeanEmbedding(lang=\"en\")\n",
    "df = pd.read_csv(pre_processed_swr_data_path)\n",
    "start = time.process_time()\n",
    "df[\"Review\"] = df[\"Review\"].apply(lambda review: embedder(\" \".join([x for x in review])))\n",
    "df.to_csv(\"../data/preprocessed/feature_generated/fasttext/fast_text_swr_features.csv\")\n",
    "display(df)\n",
    "print(time.process_time() - start)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
