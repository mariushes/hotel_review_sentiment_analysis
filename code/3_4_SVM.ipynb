{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3-1-SVM.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "gHxAO1gsnkUK",
        "bCTa1-NYn48t",
        "cOaj8AXy2yFW",
        "1mCIAU-lunwq"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKOFDC8RwFbZ",
        "colab_type": "text"
      },
      "source": [
        "# SVC and LinearSVC\n",
        "In this notebook we evaluate the different feature set with the SVC and LinearSVC each with and without upsampled feature sets. We optimize the Hyperparameters of the SVC with the Doc2Vec Pretrained and BOW TF set. The LinearSVC is optimized in another Notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3N0ade2s1Nw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1S1sYksy4vy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#package imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm, metrics, model_selection\n",
        "from ast import literal_eval\n",
        "import time\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "import statistics\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxX3akh694CU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to split train valid test with the option to upsample\n",
        "def train_test_valid_split(df,upsampling=True, print_distribution = False):\n",
        "    train,test = train_test_split(df,test_size=0.3,stratify=df[\"Reviewer_Score\"], random_state=42)\n",
        "    test,valid = train_test_split(test,test_size=0.5,stratify=test[\"Reviewer_Score\"], random_state=42)\n",
        "    #Zusammengefasst, folgende Aufteilung:\n",
        "    #70% Training, 15% Validation, 15% Test\n",
        "\n",
        "\n",
        "    unique, counts_train = np.unique(train[\"Reviewer_Score\"], return_counts=True)\n",
        "    if(upsampling): #Idee: reduce class 0 to the size of class 1, dupliate samples from class 2 to the size of class 1\n",
        "        train_0 = train[train[\"Reviewer_Score\"]==0].sample(frac=(counts_train[1]/counts_train[0]), random_state=42)\n",
        "        train_1 = train[train[\"Reviewer_Score\"]==1]\n",
        "        train_2 = train[train[\"Reviewer_Score\"]==2]\n",
        "        train = train_0.append(train_1).append(train_2)\n",
        "        train = train.sample(frac=1, random_state=42)\n",
        "    \n",
        "    if (print_distribution):\n",
        "      unique, counts_train = np.unique(train[\"Reviewer_Score\"], return_counts=True)\n",
        "      plt.bar(unique, counts_train)\n",
        "      unique, counts = np.unique(test[\"Reviewer_Score\"], return_counts=True)\n",
        "      plt.bar(unique, counts)\n",
        "      unique, counts = np.unique(valid[\"Reviewer_Score\"], return_counts=True)\n",
        "      plt.bar(unique, counts)\n",
        "      plt.title('Class Frequency')\n",
        "      plt.xlabel('Class')\n",
        "      plt.ylabel('Frequency')\n",
        "      plt.show()\n",
        "    \n",
        "    return train,valid,test\n",
        "\n",
        "# function to load all feature sets from Google Drive\n",
        "def load_from_source():\n",
        "  # list that holds a list for each category of features with the file paths for the feature data\n",
        "  # \"/fast_text/fast_text_nonswr_features.pkl\", \"/fast_text/fast_text_swr_features.pkl\"\n",
        "  #\"/doc2vec/Pretrained_withScore.csv\", \"/doc2vec/Owntrained_withScore.csv\"\n",
        "  feature_filepaths=[[\"/fast_text/fast_text_nonswr_features.pkl\", \"/fast_text/fast_text_swr_features.pkl\"],[\"/doc2vec/Pretrained_withScore.csv\", \"/doc2vec/Owntrained_withScore.csv\"],[\"/BOW/tf_561-woerter.pkl\", \"/BOW/tfidf_561-woerter.pkl\"]]\n",
        "  # list that holds a list for each category of features with the labels for the feature data, fill this in the same way the filepath array is filled\n",
        "  feature_labels = [[\"fast text without stop-word removal\", \"fast text with stop-word removal\"],[\"Doc2Vec Pretrained\",\"Doc2Vec Owntrained\"],[]]\n",
        "  # Load the dataframes and safe them in the same structure like the filepath and labels\n",
        "  dataframes = []\n",
        "  for feature_type_filepaths in feature_filepaths:\n",
        "    feature_type_dataframes = []\n",
        "    for feature_filepath in feature_type_filepaths:\n",
        "      if feature_filepath[-3:] == \"csv\":\n",
        "        df =  pd.read_csv(\"/content/drive/My Drive/Feature_generated_sets\" + feature_filepath)\n",
        "        if 'Unnamed: 0' in df.columns:\n",
        "          df = df.drop('Unnamed: 0', 1)\n",
        "      if feature_filepath[-3:] == \"pkl\":\n",
        "        df =  pd.read_pickle(\"/content/drive/My Drive/Feature_generated_sets\" + feature_filepath)\n",
        "        if 'Unnamed: 0' in df.columns:\n",
        "          df = df.drop('Unnamed: 0', 1)\n",
        "      feature_type_dataframes.append(df)\n",
        "    dataframes.append(feature_type_dataframes)\n",
        "  return dataframes\n",
        "\n",
        "# function to split all dataframes with the option of a reduced test size and passing of upsampling and drop class \"ok\" to the split function\n",
        "def split_dataframes(dataframes,test_boolean=False,test_size=100000000, upsampling = False):\n",
        "  # split the dataframes with the upper method and save them in a dictonary in arrays like the filepath\n",
        "  test_samples = lambda df: df[0:test_size] if test_boolean else df\n",
        "  split_dataframes = [] \n",
        "  for feature_type_dataframes in dataframes:\n",
        "    feature_type_split_dataframes = []\n",
        "    for feature_data in feature_type_dataframes:\n",
        "      train, valid, test = train_test_valid_split(feature_data, upsampling=upsampling)\n",
        "      train, valid, test = test_samples(train),test_samples(valid), test_samples(test)\n",
        "      feature_type_split_dataframes.append({\"train\": train, \"valid\": valid, \"test\":test}) \n",
        "\n",
        "    split_dataframes.append(feature_type_split_dataframes)\n",
        "\n",
        "  return split_dataframes\n",
        "\n",
        "# function to transform the data to feature and label numpy arrays if their are split\n",
        "def trans_to_numpy_split(split_dataframes):\n",
        "  # transform data to numpy arrays\n",
        "  split_types = [\"train\", \"valid\", \"test\"]\n",
        "  for feature_type_dataframes in split_dataframes:\n",
        "    for feature_data in feature_type_dataframes:\n",
        "      if len(feature_data[\"train\"].columns) == 2:\n",
        "        for st in split_types:\n",
        "          features = np.array(feature_data[st][\"Review\"].tolist())\n",
        "          label = np.array(feature_data[st][\"Reviewer_Score\"].tolist())\n",
        "          feature_data[st] = {\"features\": features, \"label\": label}\n",
        "      elif len(feature_data[\"train\"].columns) == 301:\n",
        "        for st in split_types:\n",
        "          features = np.array(feature_data[st].loc[:, :'299'].values)\n",
        "          label = np.array(feature_data[st][\"Reviewer_Score\"].values)\n",
        "          feature_data[st] = {\"features\": features, \"label\": label}\n",
        "      elif len(feature_data[\"train\"].columns) == 562:\n",
        "        for st in split_types:\n",
        "          features = np.array(feature_data[st].loc[:, :'yet'].values)\n",
        "          label = np.array(feature_data[st][\"Reviewer_Score\"].values)\n",
        "          feature_data[st] = {\"features\": features, \"label\": label}\n",
        "  return split_dataframes\n",
        "\n",
        "# function to transform the data to feature and label numpy arrays if their are not split\n",
        "def trans_to_numpy_unsplit(dataframes, test, test_size):\n",
        "  # transform data to numpy arrays\n",
        "  df_result = []\n",
        "  for feature_type_dataframes in dataframes:\n",
        "    df_types = []\n",
        "    for feature_data in feature_type_dataframes:\n",
        "      if test:\n",
        "        feature_data = feature_data[:test_size]\n",
        "      if len(feature_data.columns) == 2:\n",
        "        features = np.array(feature_data[\"Review\"].tolist())\n",
        "        label = np.array(feature_data[\"Reviewer_Score\"].tolist())\n",
        "        feature_data = {\"features\": features, \"label\": label}\n",
        "      elif len(feature_data.columns) == 301:\n",
        "        features = np.array(feature_data.loc[:, :'299'].values)\n",
        "        label = np.array(feature_data[\"Reviewer_Score\"].values)\n",
        "        feature_data = {\"features\": features, \"label\": label}\n",
        "      elif len(feature_data.columns) == 562:\n",
        "        features = np.array(feature_data.loc[:, :'yet'].values)\n",
        "        label = np.array(feature_data[\"Reviewer_Score\"].values)\n",
        "        feature_data = {\"features\": features, \"label\": label}\n",
        "      df_types.append(feature_data)\n",
        "    df_result.append(df_types)\n",
        "  return df_result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6Dm0oCnqL-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load all dataframes from google drive\n",
        "dataframes = load_from_source()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_J_Y3DlT79kq",
        "colab_type": "text"
      },
      "source": [
        "## SVC\n",
        "We can not use the full feature set but only a reduced set of maximum 100,000 samples in the train set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHxAO1gsnkUK",
        "colab_type": "text"
      },
      "source": [
        "### Test of different Feature sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuRGqRHRa2d3",
        "colab_type": "code",
        "outputId": "374cd313-fdcd-4a2f-bed3-8c81a3965ec9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Testing the different feature sets with the default classifier without upsampling in a 4-fold cv\n",
        "# Output: average f1 scores for each of the six feature sets\n",
        "trans_data = trans_to_numpy_unsplit(dataframes, True, 50000)\n",
        "rbf = svm.SVC()\n",
        "s1 = []\n",
        "for d1 in trans_data:\n",
        "  s2 = []\n",
        "  for d2 in d1:\n",
        "    scores = model_selection.cross_val_score(rbf, d2[\"features\"], d2[\"label\"], cv=4, n_jobs=-1, scoring=\"f1_macro\")\n",
        "    print(scores)\n",
        "    s2.append(statistics.mean(scores))\n",
        "  s1.append(s2)\n",
        "print(s1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.24536569 0.24536569 0.24536569 0.24536569]\n",
            "[0.24536569 0.24536569 0.24536569 0.24536569]\n",
            "[0.55218979 0.51703084 0.55200055 0.55623351]\n",
            "[0.24862439 0.24928277 0.24741545 0.24741233]\n",
            "[0.56658276 0.52766768 0.55206807 0.56140395]\n",
            "[0.56488821 0.5305744  0.55327771 0.55720504]\n",
            "[[0.24536568924839908, 0.24536568924839908], [0.5443636747638891, 0.2481837354326099], [0.5519306122663477, 0.5514863392099774]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYkDF2xJzDTn",
        "colab_type": "code",
        "outputId": "daf1782d-2c06-4386-8b89-b1fa969ae573",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Testing the different feature sets with the default classifier upsampling and then tested on not upsampled test set\n",
        "# Output:  f1 scores for each of the six feature sets on the test set\n",
        "dicts = trans_to_numpy_split(split_dataframes(dataframes, True, 50000, upsampling=True))\n",
        "rbf = svm.SVC()\n",
        "for d1 in dicts:\n",
        "  for d2 in d1:\n",
        "    rbf.fit(d2[\"train\"][\"features\"], d2[\"train\"][\"label\"])\n",
        "    prediction = rbf.predict(d2[\"test\"][\"features\"])\n",
        "    print(metrics.classification_report(d2[\"test\"][\"label\"], prediction))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.51      0.61     28423\n",
            "           1       0.32      0.21      0.25     13182\n",
            "           2       0.27      0.72      0.39      8395\n",
            "\n",
            "    accuracy                           0.47     50000\n",
            "   macro avg       0.45      0.48      0.42     50000\n",
            "weighted avg       0.56      0.47      0.48     50000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.52      0.61     28423\n",
            "           1       0.32      0.22      0.26     13182\n",
            "           2       0.27      0.69      0.39      8395\n",
            "\n",
            "    accuracy                           0.47     50000\n",
            "   macro avg       0.45      0.48      0.42     50000\n",
            "weighted avg       0.56      0.47      0.48     50000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.65      0.73     28423\n",
            "           1       0.40      0.51      0.45     13182\n",
            "           2       0.52      0.68      0.59      8395\n",
            "\n",
            "    accuracy                           0.62     50000\n",
            "   macro avg       0.58      0.61      0.59     50000\n",
            "weighted avg       0.67      0.62      0.63     50000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.36      0.45     28423\n",
            "           1       0.27      0.37      0.31     13182\n",
            "           2       0.18      0.29      0.22      8395\n",
            "\n",
            "    accuracy                           0.35     50000\n",
            "   macro avg       0.34      0.34      0.33     50000\n",
            "weighted avg       0.43      0.35      0.37     50000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.65      0.73     28423\n",
            "           1       0.40      0.50      0.44     13182\n",
            "           2       0.51      0.68      0.58      8395\n",
            "\n",
            "    accuracy                           0.62     50000\n",
            "   macro avg       0.58      0.61      0.59     50000\n",
            "weighted avg       0.66      0.62      0.63     50000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.64      0.73     28423\n",
            "           1       0.40      0.51      0.45     13182\n",
            "           2       0.51      0.69      0.59      8395\n",
            "\n",
            "    accuracy                           0.62     50000\n",
            "   macro avg       0.58      0.61      0.59     50000\n",
            "weighted avg       0.67      0.62      0.63     50000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCTa1-NYn48t",
        "colab_type": "text"
      },
      "source": [
        "### Hyperparameter optimization without upsampling for Doc2Vec pretrained and BOW TF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sf-NKW7FgtCz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First GridSearch with many parameters\n",
        "# Output: best parameters and a classification report on the test set\n",
        "dicts = trans_to_numpy_split(split_dataframes(dataframes, True, 5000))\n",
        "train_x, train_y, valid_x, valid_y, test_x, test_y = dicts[2][0][\"train\"][\"features\"],dicts[2][0][\"train\"][\"label\"], dicts[2][0][\"valid\"][\"features\"],dicts[2][0][\"valid\"][\"label\"], dicts[2][0][\"test\"][\"features\"],dicts[2][0][\"test\"][\"label\"]\n",
        "svc = svm.SVC()\n",
        "param_grid = {'C': [0.01, 0.1, 1, 10], 'kernel': [\"rbf\", \"poly\"], \"degree\": [1,2,3,5], \"gamma\": [0.1, 1, 10], \"decision_function_shape\": [\"ovr\", \"ovo\"]}\n",
        "grid_svm = model_selection.GridSearchCV(svc,\n",
        "                    param_grid= param_grid, \n",
        "                    scoring=\"f1_macro\",\n",
        "                    cv=4,   \n",
        "                    n_jobs=-1) \n",
        "print(\"Training now\")\n",
        "time1 = time.time()\n",
        "grid_svm.fit(train_x, train_y)\n",
        "print(time.time()-time1)\n",
        "# from https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\n",
        "print(\"Best parameters set found on development set:\")\n",
        "print()\n",
        "print(grid_svm.best_params_)\n",
        "print()\n",
        "print(\"Detailed classification report:\")\n",
        "print()\n",
        "y_true, y_pred = test_y, grid_svm.predict(test_x)\n",
        "print(metrics.classification_report(y_true, y_pred))\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65ykbl_J3kgo",
        "colab_type": "code",
        "outputId": "8b5948e6-360b-444d-9920-746d4326b7a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "# Grid search with reduced parameters\n",
        "# Output: best parameters and a classification report on the test set\n",
        "dicts = trans_to_numpy_split(split_dataframes(dataframes, True, 20000))\n",
        "train_x, train_y, valid_x, valid_y, test_x, test_y = dicts[2][0][\"train\"][\"features\"],dicts[2][0][\"train\"][\"label\"], dicts[2][0][\"valid\"][\"features\"],dicts[2][0][\"valid\"][\"label\"], dicts[2][0][\"test\"][\"features\"],dicts[2][0][\"test\"][\"label\"]\n",
        "svc = svm.SVC()\n",
        "param_grid = {'C': [0.01, 0.1, 1, 10], 'kernel': [\"rbf\"], \"gamma\": [0.1, 1, 10], \"decision_function_shape\": [\"ovr\"]}\n",
        "grid_svm = model_selection.GridSearchCV(svc,\n",
        "                    param_grid= param_grid, \n",
        "                    scoring=\"f1_macro\",\n",
        "                    cv=4,   \n",
        "                    n_jobs=-1) \n",
        "print(\"Training now\")\n",
        "time1 = time.time()\n",
        "grid_svm.fit(train_x, train_y)\n",
        "print(time.time()-time1)\n",
        "# from https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\n",
        "print(\"Best parameters set found on development set:\")\n",
        "print()\n",
        "print(grid_svm.best_params_)\n",
        "print()\n",
        "print(\"Detailed classification report:\")\n",
        "print()\n",
        "y_true, y_pred = test_y, grid_svm.predict(test_x)\n",
        "print(metrics.classification_report(y_true, y_pred))\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training now\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "9335.739322423935\n",
            "Best parameters set found on development set:\n",
            "\n",
            "{'C': 10, 'decision_function_shape': 'ovr', 'gamma': 1, 'kernel': 'rbf'}\n",
            "\n",
            "Detailed classification report:\n",
            "\n",
            "The model is trained on the full development set.\n",
            "The scores are computed on the full evaluation set.\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.82      0.77     11400\n",
            "           1       0.40      0.33      0.36      5234\n",
            "           2       0.58      0.48      0.52      3366\n",
            "\n",
            "    accuracy                           0.63     20000\n",
            "   macro avg       0.57      0.54      0.55     20000\n",
            "weighted avg       0.61      0.63      0.62     20000\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhIblIaZKCZq",
        "colab_type": "code",
        "outputId": "b209273a-e38e-4e75-8ff4-68fae604b6ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# run doc2vec pretrained with best params {'C': 0.01, 'decision_function_shape': 'ovr', 'degree': 2, 'gamma': 1, 'kernel': 'poly'} on a larger dataset\n",
        "# Output: Classification report of the test set classified with the optimized SVC\n",
        "dicts = trans_to_numpy_split(split_dataframes(dataframes, True, 100000))\n",
        "train_x, train_y, valid_x, valid_y, test_x, test_y = dicts[1][0][\"train\"][\"features\"],dicts[1][0][\"train\"][\"label\"], dicts[1][0][\"valid\"][\"features\"],dicts[1][0][\"valid\"][\"label\"], dicts[1][0][\"test\"][\"features\"],dicts[1][0][\"test\"][\"label\"]\n",
        "svc = svm.SVC(C=0.01, decision_function_shape = \"ovr\", degree=2, gamma=1, kernel=\"poly\")\n",
        "t0 = time.time()\n",
        "svc.fit(train_x, train_y)\n",
        "t1 = time.time()\n",
        "prediction = svc.predict(test_x)\n",
        "t2 = time.time()\n",
        "time_train = t1-t0\n",
        "time_predict = t2-t1\n",
        "print(time_train)\n",
        "print(time_predict)\n",
        "# results\n",
        "report = metrics.classification_report(test_y, prediction, output_dict=True)\n",
        "print(report)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9195.91665816307\n",
            "838.8839845657349\n",
            "[0 0 0 ... 1 0 0]\n",
            "{'0': {'precision': 0.7111163909595795, 'recall': 0.837261346947339, 'f1-score': 0.7690504103165298, 'support': 28598}, '1': {'precision': 0.38333333333333336, 'recall': 0.29460039883417705, 'f1-score': 0.3331598577500217, 'support': 13038}, '2': {'precision': 0.5909018861943256, 'recall': 0.44571975131516023, 'f1-score': 0.5081442104545764, 'support': 8364}, 'accuracy': 0.63026, 'macro avg': {'precision': 0.5617838701624128, 'recall': 0.5258604990322254, 'f1-score': 0.5367848261737093, 'support': 50000}, 'weighted avg': {'precision': 0.6055341984958279, 'recall': 0.63026, 'f1-score': 0.6117432007163796, 'support': 50000}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usYDcpZeKOLN",
        "colab_type": "code",
        "outputId": "fe341e78-b697-4824-db5f-2df524c589fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# run tf  with best params {'C': 10, 'decision_function_shape': 'ovr', 'gamma': 1, 'kernel': 'rbf'} on a larger dataset\n",
        "# Output: Classification report of the test set classified with the optimized SVC\n",
        "dicts = trans_to_numpy_split(split_dataframes(dataframes, True, 50000))\n",
        "train_x, train_y, valid_x, valid_y, test_x, test_y = dicts[2][0][\"train\"][\"features\"],dicts[2][0][\"train\"][\"label\"], dicts[2][0][\"valid\"][\"features\"],dicts[2][0][\"valid\"][\"label\"], dicts[2][0][\"test\"][\"features\"],dicts[2][0][\"test\"][\"label\"]\n",
        "svc = svm.SVC(C=10, decision_function_shape = \"ovr\", gamma=1, kernel=\"rbf\")\n",
        "t0 = time.time()\n",
        "svc.fit(train_x, train_y)\n",
        "t1 = time.time()\n",
        "prediction = svc.predict(test_x)\n",
        "t2 = time.time()\n",
        "time_train = t1-t0\n",
        "time_predict = t2-t1\n",
        "print(time_train)\n",
        "print(time_predict)\n",
        "# results\n",
        "print(prediction)\n",
        "report = metrics.classification_report(test_y, prediction, output_dict=True)\n",
        "print(report)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16844.78852534294\n",
            "1804.0371339321136\n",
            "[0 0 0 ... 2 1 0]\n",
            "{'0': {'precision': 0.7298809263368441, 'recall': 0.8166305336037485, 'f1-score': 0.7708226751382128, 'support': 28598}, '1': {'precision': 0.39752860411899316, 'recall': 0.3331032366927443, 'f1-score': 0.36247548303634775, 'support': 13038}, '2': {'precision': 0.5840632947160215, 'recall': 0.49426111908177905, 'f1-score': 0.5354228726848854, 'support': 8364}, 'accuracy': 0.63662, 'macro avg': {'precision': 0.5704909417239529, 'recall': 0.5479982964594239, 'f1-score': 0.5562403436198152, 'support': 50000}, 'weighted avg': {'precision': 0.618824361377786, 'recall': 0.63662, 'f1-score': 0.6249643823713379, 'support': 50000}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOaj8AXy2yFW",
        "colab_type": "text"
      },
      "source": [
        "## LinearSVC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mCIAU-lunwq",
        "colab_type": "text"
      },
      "source": [
        "### Test of different Feature sets\n",
        "We are testing all feature sets with the LinearSVC with and without upsampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0917vO00X-O",
        "colab_type": "code",
        "outputId": "c4412dc0-62b0-4a64-f576-e1fcf6fb692c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Testing the different feature sets not upsampled\n",
        "# Output: average f1 scores for each of the six feature sets\n",
        "trans_data = trans_to_numpy_unsplit(dataframes, True, 200000)\n",
        "linear_classifier = svm.LinearSVC()\n",
        "s1 = []\n",
        "for d1 in trans_data:\n",
        "  s2 = []\n",
        "  for d2 in d1:\n",
        "    scores = model_selection.cross_val_score(linear_classifier, d2[\"features\"], d2[\"label\"], cv=4, n_jobs=-1, scoring=\"f1_macro\")\n",
        "    print(scores)\n",
        "    s2.append(statistics.mean(scores))\n",
        "  s1.append(s2)\n",
        "print(s1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.3125768  0.31807336 0.31512314 0.31833965]\n",
            "[0.32414877 0.32448252 0.32557382 0.32631602]\n",
            "[0.52938076 0.52751034 0.53868915 0.53312427]\n",
            "[0.23897628 0.23897628 0.23898177 0.23898177]\n",
            "[0.54947739 0.55358653 0.56052272 0.55743804]\n",
            "[0.54788742 0.55181126 0.55606958 0.5556378 ]\n",
            "[[0.3160282380401167, 0.32513028271353456], [0.5321761311907596, 0.23897902404196636], [0.5552561696142201, 0.5528515122517301]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iB0Ib9eqfWw6",
        "colab_type": "code",
        "outputId": "79178b96-3aee-4f92-94cc-8a4905891e3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Testing the different feature sets upsampled\n",
        "# Output: f1 scores for each of the six feature sets on the test set\n",
        "dicts = trans_to_numpy_split(split_dataframes(dataframes, upsampling=True))\n",
        "svc = svm.LinearSVC()\n",
        "for d1 in dicts:\n",
        "  for d2 in d1:\n",
        "    svc.fit(d2[\"train\"][\"features\"], d2[\"train\"][\"label\"])\n",
        "    prediction = svc.predict(d2[\"test\"][\"features\"])\n",
        "    print(metrics.classification_report(d2[\"test\"][\"label\"], prediction))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.59      0.65     44096\n",
            "           1       0.35      0.18      0.24     20237\n",
            "           2       0.28      0.68      0.40     13028\n",
            "\n",
            "    accuracy                           0.50     77361\n",
            "   macro avg       0.45      0.48      0.43     77361\n",
            "weighted avg       0.55      0.50      0.50     77361\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.59      0.65     44096\n",
            "           1       0.34      0.19      0.24     20237\n",
            "           2       0.29      0.66      0.40     13028\n",
            "\n",
            "    accuracy                           0.50     77361\n",
            "   macro avg       0.45      0.48      0.43     77361\n",
            "weighted avg       0.55      0.50      0.50     77361\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.73      0.76     44096\n",
            "           1       0.41      0.34      0.37     20237\n",
            "           2       0.47      0.72      0.56     13028\n",
            "\n",
            "    accuracy                           0.62     77361\n",
            "   macro avg       0.56      0.60      0.57     77361\n",
            "weighted avg       0.64      0.62      0.62     77361\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.37      0.45     44096\n",
            "           1       0.26      0.29      0.28     20237\n",
            "           2       0.17      0.35      0.23     13028\n",
            "\n",
            "    accuracy                           0.35     77361\n",
            "   macro avg       0.34      0.34      0.32     77361\n",
            "weighted avg       0.43      0.35      0.37     77361\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.70      0.75     44096\n",
            "           1       0.41      0.38      0.39     20237\n",
            "           2       0.47      0.74      0.58     13028\n",
            "\n",
            "    accuracy                           0.62     77361\n",
            "   macro avg       0.56      0.61      0.57     77361\n",
            "weighted avg       0.65      0.62      0.63     77361\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.70      0.75     44096\n",
            "           1       0.41      0.38      0.39     20237\n",
            "           2       0.47      0.74      0.57     13028\n",
            "\n",
            "    accuracy                           0.62     77361\n",
            "   macro avg       0.56      0.61      0.57     77361\n",
            "weighted avg       0.65      0.62      0.63     77361\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}