{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec_and_TFIDF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3N0ade2s1Nw",
        "colab_type": "code",
        "outputId": "d6225685-571c-4f17-fa21-13b5dcad945a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3EyP2FSFbNK",
        "colab_type": "text"
      },
      "source": [
        "#Conduct word2vec and symbolic feature experiments in combination with Linear SVC\n",
        "* Linear SVC experiments are conducted in this notebook with Doc2Vec features and BOW TF TFIDF and balanced versions of those.\n",
        "* Hyperparameter grid search tuning in combination with a k-fold cross validation. \n",
        "* Note: You need to get in reach of a feature generated set to run this notebook.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ43UxG1xf0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = [] #RAM trick google Colab\n",
        "while(1):\n",
        "  a.append('1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1S1sYksy4vy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd #import \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm, metrics, model_selection\n",
        "from ast import literal_eval\n",
        "from sklearn.metrics import classification_report\n",
        "import time\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "import statistics\n",
        "from sklearn.svm import LinearSVC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxX3akh694CU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_test_valid_split(df,upsampling, print_distribution = False): #split train, valid and test reasonably, balance the set if declared so\n",
        "    train,test = train_test_split(df,test_size=0.3,stratify=df[\"Reviewer_Score\"], random_state=42)\n",
        "    test,valid = train_test_split(test,test_size=0.5,stratify=test[\"Reviewer_Score\"], random_state=42)\n",
        "    #Zusammengefasst, folgende Aufteilung:\n",
        "    #70% Training, 15% Validation, 15% Test\n",
        "\n",
        "\n",
        "    unique, counts_train = np.unique(train[\"Reviewer_Score\"], return_counts=True)\n",
        "    if(upsampling): #Idee: reduce class 0 to the size of class 1, dupliate samples from class 2 to the size of class 1\n",
        "        train_0 = train[train[\"Reviewer_Score\"]==0].sample(frac=(counts_train[1]/counts_train[0]), random_state=42)\n",
        "        train_1 = train[train[\"Reviewer_Score\"]==1]\n",
        "        train_2 = train[train[\"Reviewer_Score\"]==2].sample(frac=(counts_train[1]/counts_train[2]),replace=True, random_state=42)\n",
        "        train = train_0.append(train_1).append(train_2)\n",
        "        train = train.sample(frac=1, random_state=42)\n",
        "    \n",
        "    if (print_distribution):\n",
        "      unique, counts_train = np.unique(train[\"Reviewer_Score\"], return_counts=True)\n",
        "      plt.bar(unique, counts_train)\n",
        "      unique, counts = np.unique(test[\"Reviewer_Score\"], return_counts=True)\n",
        "      plt.bar(unique, counts)\n",
        "      unique, counts = np.unique(valid[\"Reviewer_Score\"], return_counts=True)\n",
        "      plt.bar(unique, counts)\n",
        "      plt.title('Class Frequency')\n",
        "      plt.xlabel('Class')\n",
        "      plt.ylabel('Frequency')\n",
        "      plt.show()\n",
        "    \n",
        "    return train,valid,test\n",
        "\n",
        "def load_from_source(): #load data\n",
        "  # list that holds a list for each category of features with the file paths for the feature data\n",
        "  # \"/BOW/tf-idf-set-0-005-tokenization-stpwrds-stemming.csv\", \"/BOW/tf-idf-set-0-005-tokenization-stpwrds-stemming-ngram.csv\"\n",
        "  feature_filepaths=[[\"/BOW/tf_561-woerter.pkl\", \"/BOW/tfidf_561-woerter.pkl\"],[\"/doc2vec/Pretrained_withScore.csv\", \"/doc2vec/Owntrained_withScore.csv\"]]\n",
        "  # list that holds a list for each category of features with the labels for the feature data, fill this in the same way the filepath array is filled\n",
        "  feature_labels = [[\"fast text without stop-word removal\", \"fast text with stop-word removal\"],[\"Doc2Vec Pretrained\",\"Doc2Vec Owntrained\"]]\n",
        "  # Load the dataframes and safe them in the same structure like the filepath and labels\n",
        "  dataframes = []\n",
        "  for feature_type_filepaths in feature_filepaths:\n",
        "    feature_type_dataframes = []\n",
        "    for feature_filepath in feature_type_filepaths:\n",
        "      if feature_filepath[-3:] == \"csv\":\n",
        "        df =  pd.read_csv(\"/content/drive/My Drive/Feature_generated_sets\" + feature_filepath)\n",
        "        if 'Unnamed: 0' in df.columns:\n",
        "          df = df.drop('Unnamed: 0', 1)\n",
        "      if feature_filepath[-3:] == \"pkl\":\n",
        "        df =  pd.read_pickle(\"/content/drive/My Drive/Feature_generated_sets\" + feature_filepath)\n",
        "        if 'Unnamed: 0' in df.columns:\n",
        "          df = df.drop('Unnamed: 0', 1)\n",
        "      feature_type_dataframes.append(df)\n",
        "    dataframes.append(feature_type_dataframes)\n",
        "  return dataframes\n",
        "\n",
        "\n",
        "def split_dataframes(dataframes,upsampling,test_boolean=False,test_size=100000000): #organizing the whole structure in arrays and dictionaries\n",
        "  # split the dataframes with the upper method and save them in a dictonary in arrays like the filepath\n",
        "  test_samples = lambda df: df[0:test_size] if test_boolean else df\n",
        "  split_dataframes = [] \n",
        "  for feature_type_dataframes in dataframes:\n",
        "    feature_type_split_dataframes = []\n",
        "    for feature_data in feature_type_dataframes:\n",
        "      train, valid, test = train_test_valid_split(feature_data, upsampling)\n",
        "      train, valid, test = test_samples(train),test_samples(valid), test_samples(test)\n",
        "      feature_type_split_dataframes.append({\"train\": train, \"valid\": valid, \"test\":test}) \n",
        "\n",
        "    split_dataframes.append(feature_type_split_dataframes)\n",
        "\n",
        "  return split_dataframes\n",
        "\n",
        "def trans_to_numpy_split(split_dataframes): #correcting data formats if necesarry for sklearn\n",
        "  # transform data to numpy arrays\n",
        "  split_types = [\"train\", \"valid\", \"test\"]\n",
        "  for feature_type_dataframes in split_dataframes:\n",
        "    for feature_data in feature_type_dataframes:\n",
        "      if len(feature_data[\"train\"].columns) == 2:\n",
        "        for st in split_types:\n",
        "          features = np.array(feature_data[st][\"Review\"].tolist())\n",
        "          label = np.array(feature_data[st][\"Reviewer_Score\"].tolist())\n",
        "          feature_data[st] = {\"features\": features, \"label\": label}\n",
        "      elif len(feature_data[\"train\"].columns) == 301:\n",
        "        for st in split_types:\n",
        "          features = np.array(feature_data[st].loc[:, :'299'].values)\n",
        "          label = np.array(feature_data[st][\"Reviewer_Score\"].values)\n",
        "          feature_data[st] = {\"features\": features, \"label\": label}\n",
        "      elif len(feature_data[\"train\"].columns) == 562:\n",
        "        for st in split_types:\n",
        "          features = np.array(feature_data[st].loc[:, :'yet'].values)\n",
        "          label = np.array(feature_data[st][\"Reviewer_Score\"].values)\n",
        "          feature_data[st] = {\"features\": features, \"label\": label}\n",
        "  return split_dataframes\n",
        "\n",
        "def trans_to_numpy_unsplit(dataframes, test, test_size): #assisting method to reorganize data formats\n",
        "  # transform data to numpy arrays\n",
        "  df_result = []\n",
        "  for feature_type_dataframes in dataframes:\n",
        "    df_types = []\n",
        "    for feature_data in feature_type_dataframes:\n",
        "      if test:\n",
        "        feature_data = feature_data[:test_size]\n",
        "      if len(feature_data.columns) == 2:\n",
        "        features = np.array(feature_data[\"Review\"].tolist())\n",
        "        label = np.array(feature_data[\"Reviewer_Score\"].tolist())\n",
        "        feature_data = {\"features\": features, \"label\": label}\n",
        "      elif len(feature_data.columns) == 301:\n",
        "        features = np.array(feature_data.loc[:, :'299'].values)\n",
        "        label = np.array(feature_data[\"Reviewer_Score\"].values)\n",
        "        feature_data = {\"features\": features, \"label\": label}\n",
        "      elif len(feature_data.columns) == 562:\n",
        "        features = np.array(feature_data.loc[:, :'yet'].values)\n",
        "        label = np.array(feature_data[\"Reviewer_Score\"].values)\n",
        "        feature_data = {\"features\": features, \"label\": label}\n",
        "      df_types.append(feature_data)\n",
        "    df_result.append(df_types)\n",
        "  return df_result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6Dm0oCnqL-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataframes = load_from_source()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0brwlgyFXGB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cross_validation_hyperparameter_grid_search_for_SVM(dataframes, upsampling, i, j, description, param_grid={ 'dual' : [False] ,\"tol\": [0.00001,0.0001,0.001,0.01], 'C': [0.1,5,10], \"multi_class\": [\"ovr\",\"crammer_singer\"]}): #automatic grid search implementation\n",
        "  \n",
        "  dicts = trans_to_numpy_split(split_dataframes(dataframes, upsampling, test_boolean=False, test_size=20000))\n",
        "  dicts = dicts[i][j]\n",
        "  train_x, train_y, valid_x, valid_y, test_x, test_y = dicts[\"train\"][\"features\"],dicts[\"train\"][\"label\"], dicts[\"valid\"][\"features\"],dicts[\"valid\"][\"label\"], dicts[\"test\"][\"features\"],dicts[\"test\"][\"label\"]\n",
        "\n",
        "\n",
        "\n",
        "  svc = svm.LinearSVC()\n",
        "  param_grid = { 'dual' : [False] ,\"tol\": [0.00001,0.0001,0.001,0.01], 'C': [0.1,5,10], \"multi_class\": [\"ovr\",\"crammer_singer\"]}\n",
        "  grid_svm = model_selection.GridSearchCV(svc,\n",
        "                      param_grid= param_grid, \n",
        "                      scoring=\"f1_macro\",\n",
        "                      cv=4,   \n",
        "                      n_jobs=-1) \n",
        "  print(\"Training now: \",description)\n",
        "  print(\"Balanced: \",upsampling)\n",
        "  time1 = time.time()\n",
        "  grid_svm.fit(train_x, train_y)\n",
        "  print(time.time()-time1)\n",
        "  # from https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\n",
        "  print(\"Best parameters set found on development set:\")\n",
        "  print(grid_svm.best_params_,\"\\n\")\n",
        "  \n",
        "  print(\"The model is trained on the full development set.\")\n",
        "  print(\"The scores are computed on the full evaluation set.\\n\")\n",
        "  print(\"Detailed classification report:\")\n",
        "  y_true, y_pred = test_y, grid_svm.predict(test_x)\n",
        "  print(metrics.classification_report(y_true, y_pred),\"\\n\")\n",
        "  print(\"-----------------------------------------------------\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idgrnTyZ9B0A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bK-NcCTSFDSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzsErI7F1oUQ",
        "colab_type": "text"
      },
      "source": [
        "Training now:  /BOW/tf_561-woerter.pkl\n",
        "Balanced:  True\n",
        "972.0990505218506\n",
        "Best parameters set found on development set:\n",
        "{'C': 5, 'dual': False, 'multi_class': 'ovr', 'tol': 0.01} \n",
        "\n",
        "The model is trained on the full development set.\n",
        "The scores are computed on the full evaluation set.\n",
        "\n",
        "Detailed classification report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.81      0.70      0.75     44096\n",
        "           1       0.41      0.38      0.39     20237\n",
        "           2       0.47      0.74      0.58     13028\n",
        "\n",
        "    accuracy                           0.62     77361\n",
        "   macro avg       0.56      0.61      0.58     77361\n",
        "weighted avg       0.65      0.62      0.63     77361\n",
        " \n",
        "\n",
        "-----------------------------------------------------\n",
        "Training now:  /BOW/tf_561-woerter.pkl\n",
        "Balanced:  False\n",
        "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
        "  \"timeout or by a memory leak.\", UserWarning\n",
        "1156.450232744217\n",
        "Best parameters set found on development set:\n",
        "{'C': 10, 'dual': False, 'multi_class': 'ovr', 'tol': 0.001} \n",
        "\n",
        "The model is trained on the full development set.\n",
        "The scores are computed on the full evaluation set.\n",
        "\n",
        "Detailed classification report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.71      0.91      0.80     44096\n",
        "           1       0.46      0.19      0.27     20237\n",
        "           2       0.60      0.56      0.58     13028\n",
        "\n",
        "    accuracy                           0.66     77361\n",
        "   macro avg       0.59      0.55      0.55     77361\n",
        "weighted avg       0.62      0.66      0.62     77361\n",
        " \n",
        "\n",
        "-----------------------------------------------------\n",
        "Training now:  /BOW/tfidf_561-woerter.pkl\n",
        "Balanced:  True\n",
        "956.3361351490021\n",
        "Best parameters set found on development set:\n",
        "{'C': 0.1, 'dual': False, 'multi_class': 'ovr', 'tol': 0.001} \n",
        "\n",
        "The model is trained on the full development set.\n",
        "The scores are computed on the full evaluation set.\n",
        "\n",
        "Detailed classification report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.82      0.70      0.75     44096\n",
        "           1       0.41      0.38      0.39     20237\n",
        "           2       0.47      0.74      0.57     13028\n",
        "\n",
        "    accuracy                           0.62     77361\n",
        "   macro avg       0.56      0.61      0.57     77361\n",
        "weighted avg       0.65      0.62      0.63     77361\n",
        " \n",
        "\n",
        "-----------------------------------------------------\n",
        "Training now:  /BOW/tfidf_561-woerter.pkl\n",
        "Balanced:  False\n",
        "1132.4396421909332\n",
        "Best parameters set found on development set:\n",
        "{'C': 5, 'dual': False, 'multi_class': 'ovr', 'tol': 0.001} \n",
        "\n",
        "The model is trained on the full development set.\n",
        "The scores are computed on the full evaluation set.\n",
        "\n",
        "Detailed classification report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.70      0.91      0.80     44096\n",
        "           1       0.46      0.18      0.26     20237\n",
        "           2       0.59      0.56      0.57     13028\n",
        "\n",
        "    accuracy                           0.66     77361\n",
        "   macro avg       0.59      0.55      0.54     77361\n",
        "weighted avg       0.62      0.66      0.62     77361\n",
        " \n",
        "\n",
        "-----------------------------------------------------\n",
        "Training now:  /doc2vec/Pretrained_withScore.csv\n",
        "Balanced:  True\n",
        "25041.50695824623\n",
        "Best parameters set found on development set:\n",
        "{'C': 5, 'dual': False, 'multi_class': 'ovr', 'tol': 0.0001} \n",
        "\n",
        "The model is trained on the full development set.\n",
        "The scores are computed on the full evaluation set.\n",
        "\n",
        "Detailed classification report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.79      0.73      0.76     44096\n",
        "           1       0.41      0.34      0.37     20237\n",
        "           2       0.46      0.72      0.56     13028\n",
        "\n",
        "    accuracy                           0.62     77361\n",
        "   macro avg       0.56      0.59      0.57     77361\n",
        "weighted avg       0.64      0.62      0.62     77361\n",
        " \n",
        "\n",
        "-----------------------------------------------------\n",
        "Training now:  /doc2vec/Owntrained_withScore.csv\n",
        "Balanced:  True\n",
        "1420.5369265079498\n",
        "Best parameters set found on development set:\n",
        "{'C': 10, 'dual': False, 'multi_class': 'ovr', 'tol': 1e-05} \n",
        "\n",
        "The model is trained on the full development set.\n",
        "The scores are computed on the full evaluation set.\n",
        "\n",
        "Detailed classification report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.57      0.36      0.44     44096\n",
        "           1       0.26      0.30      0.28     20237\n",
        "           2       0.17      0.35      0.23     13028\n",
        "\n",
        "    accuracy                           0.34     77361\n",
        "   macro avg       0.34      0.34      0.32     77361\n",
        "weighted avg       0.42      0.34      0.36     77361\n",
        " \n",
        "\n",
        "-----------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Training now:  /doc2vec/Owntrained_withScore.csv\n",
        "Balanced:  False\n",
        "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
        "  \"timeout or by a memory leak.\", UserWarning\n",
        "1517.6253328323364\n",
        "Best parameters set found on development set:\n",
        "{'C': 0.1, 'dual': False, 'multi_class': 'ovr', 'tol': 1e-05} \n",
        "\n",
        "The model is trained on the full development set.\n",
        "The scores are computed on the full evaluation set.\n",
        "\n",
        "Detailed classification report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.57      1.00      0.73     44096\n",
        "           1       0.00      0.00      0.00     20237\n",
        "           2       0.00      0.00      0.00     13028\n",
        "\n",
        "    accuracy                           0.57     77361\n",
        "   macro avg       0.19      0.33      0.24     77361\n",
        "weighted avg       0.32      0.57      0.41     77361\n",
        " \n",
        "\n",
        "-----------------------------------------------------\n",
        "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
        "  _warn_prf(average, modifier, msg_start, len(result))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3rCPw3kFDjR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "3da311c9-76fd-48a3-91f1-291c229a0de2"
      },
      "source": [
        "# {'C': 5, 'dual': False, 'multi_class': 'ovr', 'tol': 0.0001}\n",
        "cross_validation_hyperparameter_grid_search_for_SVM(dataframes, False, 1, 0, description[1][0], param_grid = { 'dual' : [False] ,\"tol\": [0.0001], 'C': [5], \"multi_class\": [\"ovr\"]})"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training now:  /doc2vec/Pretrained_withScore.csv\n",
            "Balanced:  False\n",
            "34306.325429201126\n",
            "Best parameters set found on development set:\n",
            "{'C': 10, 'dual': False, 'multi_class': 'ovr', 'tol': 0.001} \n",
            "\n",
            "The model is trained on the full development set.\n",
            "The scores are computed on the full evaluation set.\n",
            "\n",
            "Detailed classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.92      0.79     44096\n",
            "           1       0.45      0.17      0.24     20237\n",
            "           2       0.61      0.50      0.55     13028\n",
            "\n",
            "    accuracy                           0.65     77361\n",
            "   macro avg       0.58      0.53      0.53     77361\n",
            "weighted avg       0.61      0.65      0.60     77361\n",
            " \n",
            "\n",
            "-----------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X9X2iGTFDp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIHtkrHuFD1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn_Y_AutFD_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvmuI3-NFDnI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCW9HDOAFDhN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tgbi7koeFDc0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}